{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43bb97-91fd-424b-8256-bc45393a5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21371d6-409f-47e3-9f89-a79f490f3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../initial/Train.csv\")\n",
    "test = pd.read_csv(\"../initial/Test.csv\")\n",
    "merged = pd.concat([train.set_index(\"user_id\"), test.set_index(\"user_id\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6bb602-2259-4703-aac1-fd3a39cc0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(train_y):\n",
    "    train_y.drop(\"ARPU_SEGMENT\", axis=1, inplace=True)\n",
    "    train_y.drop(\"MRG\", axis=1, inplace=True)\n",
    "    try:\n",
    "        train_y.set_index(\"user_id\", inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    target = None\n",
    "    if \"CHURN\" in train_y.columns:\n",
    "        target = train_y[\"CHURN\"]\n",
    "        train_y.drop(\"CHURN\", inplace=True, axis=1)\n",
    "    # Feature generating\n",
    "\n",
    "    # Bits\n",
    "    # Sum calls+data\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"TELE_NANS_SUM\",\n",
    "        train_y[[\"DATA_VOLUME\", \"ON_NET\", \"ORANGE\", \"TIGO\", \"ZONE1\", \"ZONE2\"]]\n",
    "        .isnull()\n",
    "        .sum(axis=1),\n",
    "    )\n",
    "    # Sum of another\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"NONTELE_NANS_SUM\",\n",
    "        train_y[\n",
    "            train_y.columns.difference(\n",
    "                [\"DATA_VOLUME\", \"ON_NET\", \"ORANGE\", \"TIGO\", \"ZONE1\", \"ZONE2\"]\n",
    "            )\n",
    "        ]\n",
    "        .isnull()\n",
    "        .sum(axis=1),\n",
    "    )\n",
    "    # Bit for REGION?\n",
    "\n",
    "    train_y.insert(\n",
    "        train_y.shape[1], \"MONTANT_TO_REVENUE\", train_y[\"MONTANT\"] / train_y[\"REVENUE\"]\n",
    "    )\n",
    "    train_y[\"MONTANT_TO_REVENUE\"].fillna(0, inplace=True)\n",
    "\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"FULL_CALLS_SUM\",\n",
    "        train_y[[\"ON_NET\", \"ORANGE\", \"TIGO\", \"ZONE1\", \"ZONE2\"]].sum(axis=1),\n",
    "    )\n",
    "    train_y[\"FULL_CALLS_SUM\"].fillna(0, inplace=True)\n",
    "\n",
    "    train_y.insert(train_y.shape[1], 'FULL_CALLS_SUM_TO_REGULATIRY', (1.0*train_y[['ON_NET', 'ORANGE', 'TIGO', 'ZONE1', 'ZONE2']].sum(axis=1))/(train_y['REGULARITY']*1.0))\n",
    "    train_y[\"FULL_CALLS_SUM_TO_REGULATIRY\"].fillna(0, inplace=True)\n",
    "\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"DATA_VOLUME_TO_REGULARITY\",\n",
    "        (\n",
    "            train_y[\"DATA_VOLUME\"].astype(np.single)\n",
    "            / (train_y[\"REGULARITY\"].astype(np.single))\n",
    "        ),\n",
    "    )\n",
    "    train_y[\"DATA_VOLUME_TO_REGULARITY\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Categorical TENURE\n",
    "    sorted_tenure = {\n",
    "        i: j for i, j in zip(np.sort(train_y[\"TENURE\"].unique()), range(8))\n",
    "    }\n",
    "    train_y[\"TENURE\"].replace(to_replace=sorted_tenure, inplace=True)\n",
    "\n",
    "    # Update 27.09\n",
    "    # The only full nans rows info\n",
    "    train_y[\"TEN_REGULARITY_MEDIAN\"] = train_y.groupby(\"TENURE\")[\"REGULARITY\"].apply(\n",
    "        lambda x: x - x.median()\n",
    "    )\n",
    "    train_y[\"TEN_REGULARITY_MEAN\"] = train_y.groupby(\"TENURE\")[\"REGULARITY\"].apply(\n",
    "        lambda x: x - x.mean()\n",
    "    )\n",
    "\n",
    "    # Categorical REGION\n",
    "    train_y[\"REGION\"].fillna(\"UNKNOWN\", inplace=True)\n",
    "    sorted_region = {\n",
    "        i: j\n",
    "        for i, j in zip(\n",
    "            train_y[\"REGION\"].value_counts().index,\n",
    "            range(len(train_y[\"REGION\"].value_counts().index)),\n",
    "        )\n",
    "    }\n",
    "    train_y[\"REGION\"].replace(to_replace=sorted_region, inplace=True)\n",
    "\n",
    "    # Update 27.09\n",
    "    train_y[\"REG_TEN_REGULARITY_MEDIAN\"] = train_y.groupby([\"REGION\", \"TENURE\"])[\n",
    "        \"REGULARITY\"\n",
    "    ].apply(lambda x: x - x.median())\n",
    "    train_y[\"REG_TEN_REGULARITY_MEAN\"] = train_y.groupby([\"REGION\", \"TENURE\"])[\n",
    "        \"REGULARITY\"\n",
    "    ].apply(lambda x: x - x.mean())\n",
    "\n",
    "    train_y[\"TOP_PACK\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    train_y[\"FREQ_TOP_PACK\"].fillna(0, inplace=True)\n",
    "    train_y[\"MONTANT\"].fillna(0, inplace=True)\n",
    "    train_y[\"FREQUENCE_RECH\"].fillna(0, inplace=True)\n",
    "    train_y[\"REVENUE\"].fillna(0, inplace=True)\n",
    "    train_y[\"FREQUENCE\"].fillna(0, inplace=True)\n",
    "    train_y[\"ON_NET\"].replace(to_replace=0, value=1, inplace=True)\n",
    "    train_y[\"ON_NET\"].fillna(0, inplace=True)\n",
    "    train_y[\"ORANGE\"].replace(to_replace=0, value=1, inplace=True)\n",
    "    train_y[\"ORANGE\"].fillna(0, inplace=True)\n",
    "    train_y[\"TIGO\"].replace(to_replace=0, value=1, inplace=True)\n",
    "    train_y[\"TIGO\"].fillna(0, inplace=True)\n",
    "    train_y[\"ZONE1\"].replace(to_replace=0, value=1, inplace=True)\n",
    "    train_y[\"ZONE1\"].fillna(0, inplace=True)\n",
    "    train_y[\"ZONE2\"].replace(to_replace=0, value=1, inplace=True)\n",
    "    train_y[\"ZONE2\"].fillna(0, inplace=True)\n",
    "    train_y[\"DATA_VOLUME\"].replace(to_replace=0, value=1, inplace=True)\n",
    "    train_y[\"DATA_VOLUME\"].fillna(0, inplace=True)\n",
    "\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"FREQ_DIFF_RELATIVE\",\n",
    "        (\n",
    "            (train_y[\"FREQUENCE\"] - train_y[\"FREQUENCE_RECH\"]) / train_y[\"FREQUENCE\"]\n",
    "        ).astype(np.single),\n",
    "    )\n",
    "    train_y[\"FREQ_DIFF_RELATIVE\"].fillna(999.0, inplace=True)\n",
    "\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_DATA_VOLUME_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"DATA_VOLUME\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_DATA_VOLUME_MEDIAN\"] = train_y[\"REGUL_DATA_VOLUME_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_DATA_VOLUME_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"DATA_VOLUME\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_DATA_VOLUME_MEAN\"] = train_y[\"REGUL_DATA_VOLUME_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FULL_CALLS_SUM_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FULL_CALLS_SUM\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_FULL_CALLS_SUM_MEDIAN\"] = train_y[\"REGUL_FULL_CALLS_SUM_MEDIAN\"].astype(\"float16\")\n",
    "    \n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FULL_CALLS_SUM_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FULL_CALLS_SUM\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_FULL_CALLS_SUM_MEAN\"] = train_y[\"REGUL_FULL_CALLS_SUM_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_TELE_NANS_SUM_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"TELE_NANS_SUM\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_TELE_NANS_SUM_MEDIAN\"] = train_y[\"REGUL_TELE_NANS_SUM_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_TELE_NANS_SUM_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"TELE_NANS_SUM\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_TELE_NANS_SUM_MEAN\"] = train_y[\"REGUL_TELE_NANS_SUM_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_NONTELE_NANS_SUM_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"NONTELE_NANS_SUM\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_NONTELE_NANS_SUM_MEDIAN\"] = train_y[\"REGUL_NONTELE_NANS_SUM_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_NONTELE_NANS_SUM_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"NONTELE_NANS_SUM\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_NONTELE_NANS_SUM_MEAN\"] = train_y[\"REGUL_NONTELE_NANS_SUM_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FREQ_TOP_PACK_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FREQ_TOP_PACK\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_FREQ_TOP_PACK_MEDIAN\"] = train_y[\"REGUL_FREQ_TOP_PACK_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FREQ_TOP_PACK_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FREQ_TOP_PACK\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_FREQ_TOP_PACK_MEAN\"] = train_y[\"REGUL_FREQ_TOP_PACK_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_MONTANT_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"MONTANT\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_MONTANT_MEDIAN\"] = train_y[\"REGUL_MONTANT_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_MONTANT_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"MONTANT\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_MONTANT_MEAN\"] = train_y[\"REGUL_MONTANT_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FREQUENCE_RECH_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FREQUENCE_RECH\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_FREQUENCE_RECH_MEDIAN\"] = train_y[\"REGUL_FREQUENCE_RECH_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FREQUENCE_RECH_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FREQUENCE_RECH\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_FREQUENCE_RECH_MEAN\"] = train_y[\"REGUL_FREQUENCE_RECH_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_REVENUE_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"REVENUE\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_REVENUE_MEDIAN\"] = train_y[\"REGUL_REVENUE_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_REVENUE_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"REVENUE\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_REVENUE_MEAN\"] = train_y[\"REGUL_REVENUE_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FREQUENCE_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FREQUENCE\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_FREQUENCE_MEDIAN\"] = train_y[\"REGUL_FREQUENCE_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_FREQUENCE_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"FREQUENCE\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_FREQUENCE_MEAN\"] = train_y[\"REGUL_FREQUENCE_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ON_NET_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ON_NET\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_ON_NET_MEDIAN\"] = train_y[\"REGUL_ON_NET_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ON_NET_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ON_NET\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_ON_NET_MEAN\"] = train_y[\"REGUL_ON_NET_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ORANGE_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ORANGE\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_ORANGE_MEDIAN\"] = train_y[\"REGUL_ORANGE_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ORANGE_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ORANGE\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_ORANGE_MEAN\"] = train_y[\"REGUL_ORANGE_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_TIGO_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"TIGO\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_TIGO_MEDIAN\"] = train_y[\"REGUL_TIGO_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_TIGO_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"TIGO\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_TIGO_MEAN\"] = train_y[\"REGUL_TIGO_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ZONE1_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ZONE1\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_ZONE1_MEDIAN\"] = train_y[\"REGUL_ZONE1_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ZONE1_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ZONE1\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_ZONE1_MEAN\"] = train_y[\"REGUL_ZONE1_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ZONE2_MEDIAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ZONE2\"].median()),\n",
    "    )\n",
    "    train_y[\"REGUL_ZONE2_MEDIAN\"] = train_y[\"REGUL_ZONE2_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGUL_ZONE2_MEAN\",\n",
    "        train_y[\"REGULARITY\"].map(train.groupby(\"REGULARITY\")[\"ZONE2\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGUL_ZONE2_MEAN\"] = train_y[\"REGUL_ZONE2_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    ###REGION###\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_DATA_VOLUME_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"DATA_VOLUME\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_DATA_VOLUME_MEDIAN\"] = train_y[\"REGION_DATA_VOLUME_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_DATA_VOLUME_MEAN\",\n",
    "         train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"DATA_VOLUME\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_DATA_VOLUME_MEAN\"] = train_y[\"REGION_DATA_VOLUME_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FULL_CALLS_SUM_MEDIAN\",\n",
    "         train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FULL_CALLS_SUM\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_FULL_CALLS_SUM_MEDIAN\"] = train_y[\"REGION_FULL_CALLS_SUM_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FULL_CALLS_SUM_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FULL_CALLS_SUM\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_FULL_CALLS_SUM_MEAN\"] = train_y[\"REGION_FULL_CALLS_SUM_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_TELE_NANS_SUM_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"TELE_NANS_SUM\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_TELE_NANS_SUM_MEDIAN\"] = train_y[\"REGION_TELE_NANS_SUM_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_TELE_NANS_SUM_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"TELE_NANS_SUM\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_TELE_NANS_SUM_MEAN\"] = train_y[\"REGION_TELE_NANS_SUM_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_NONTELE_NANS_SUM_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"NONTELE_NANS_SUM\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_NONTELE_NANS_SUM_MEDIAN\"] = train_y[\"REGION_NONTELE_NANS_SUM_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_NONTELE_NANS_SUM_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"NONTELE_NANS_SUM\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_NONTELE_NANS_SUM_MEAN\"] = train_y[\"REGION_NONTELE_NANS_SUM_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FREQ_TOP_PACK_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FREQ_TOP_PACK\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_FREQ_TOP_PACK_MEDIAN\"] = train_y[\"REGION_FREQ_TOP_PACK_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FREQ_TOP_PACK_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FREQ_TOP_PACK\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_FREQ_TOP_PACK_MEAN\"] = train_y[\"REGION_FREQ_TOP_PACK_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_MONTANT_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"MONTANT\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_MONTANT_MEDIAN\"] = train_y[\"REGION_MONTANT_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_MONTANT_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"MONTANT\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_MONTANT_MEAN\"] = train_y[\"REGION_MONTANT_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FREQUENCE_RECH_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FREQUENCE_RECH\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_FREQUENCE_RECH_MEDIAN\"] = train_y[\"REGION_FREQUENCE_RECH_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FREQUENCE_RECH_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FREQUENCE_RECH\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_FREQUENCE_RECH_MEAN\"] = train_y[\"REGION_FREQUENCE_RECH_MEAN\"].astype(\"float16\")\n",
    "\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_REVENUE_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"REVENUE\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_REVENUE_MEDIAN\"] = train_y[\"REGION_REVENUE_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_REVENUE_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"REVENUE\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_REVENUE_MEAN\"] = train_y[\"REGION_REVENUE_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FREQUENCE_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FREQUENCE\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_FREQUENCE_MEDIAN\"] = train_y[\"REGION_FREQUENCE_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_FREQUENCE_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"FREQUENCE\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_FREQUENCE_MEAN\"] = train_y[\"REGION_FREQUENCE_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_ON_NET_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ON_NET\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_ON_NET_MEDIAN\"] = train_y[\"REGION_ON_NET_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_ON_NET_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ON_NET\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_ON_NET_MEAN\"] = train_y[\"REGION_ON_NET_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_ORANGE_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ORANGE\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_ORANGE_MEDIAN\"] = train_y[\"REGION_ORANGE_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_ORANGE_MEAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ORANGE\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_ORANGE_MEAN\"] = train_y[\"REGION_ORANGE_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_TIGO_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"TIGO\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_TIGO_MEDIAN\"] = train_y[\"REGION_TIGO_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1], \"REGION_TIGO_MEAN\", train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"TIGO\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_TIGO_MEAN\"] = train_y[\"REGION_TIGO_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_ZONE1_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ZONE1\"].median()),\n",
    "    )\n",
    "    train_y[\"REGION_ZONE1_MEDIAN\"] = train_y[\"REGION_ZONE1_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1], \"REGION_ZONE1_MEAN\", train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ZONE1\"].mean()),\n",
    "    )\n",
    "    train_y[\"REGION_ZONE1_MEAN\"] = train_y[\"REGION_ZONE1_MEAN\"].astype(\"float16\")\n",
    "    # NEW!\n",
    "    train_y.insert(\n",
    "        train_y.shape[1],\n",
    "        \"REGION_ZONE2_MEDIAN\",\n",
    "        train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ZONE2\"].median())\n",
    "    )\n",
    "    train_y[\"REGION_ZONE2_MEDIAN\"] = train_y[\"REGION_ZONE2_MEDIAN\"].astype(\"float16\")\n",
    "    train_y.insert(\n",
    "        train_y.shape[1], \"REGION_ZONE2_MEAN\", train_y[\"REGION\"].map(train.groupby(\"REGION\")[\"ZONE2\"].mean())\n",
    "    )\n",
    "    train_y[\"REGION_ZONE2_MEAN\"] = train_y[\"REGION_ZONE2_MEAN\"].astype(\"float16\")\n",
    "    train_y[\"POPULARITY\"] = train_y[\"TOP_PACK\"].map(train_y[\"TOP_PACK\"].value_counts()/len(train_y[\"TOP_PACK\"]))\n",
    "\n",
    "    train_y.drop(\"TOP_PACK\", axis=1, inplace=True)\n",
    "    # Move target to last position\n",
    "    if target is not None:\n",
    "        train_y.insert(train_y.shape[1], \"CHURN\", target)\n",
    "\n",
    "    return train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f541bfa-0219-4758-958a-381f0d094363",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preproc_train = data_preparation(train)\n",
    "Preproc_train.to_csv(\"dataset_2_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5882a01-e979-4de9-9532-f1f9fcf7fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preproc_test = data_preparation(test)\n",
    "Preproc_test.to_csv(\"dataset_2_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e2e817-5860-4115-b147-296837fa3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = data_preparation(merged)\n",
    "dirty_train = train_merged.iloc[: train.shape[0]]\n",
    "dirty_test = train_merged[~train_merged.index.isin(dirty_train.index)]\n",
    "dirty_train.to_csv(\"dataset_2_train_merged.csv\")\n",
    "dirty_test.to_csv(\"dataset_2_test_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428f147-cb5a-40f8-9f52-37fadff61282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
